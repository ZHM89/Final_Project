{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92006750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"pastel\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 600 # Set high resolution for plots\n",
    "mpl.rcParams['savefig.dpi'] = 600 # Set high resolution for saved plots\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'  # Or 'notebook_connected' for JupyterLab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669ce38",
   "metadata": {},
   "source": [
    "### 1. Data Loading & Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c372b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('insurance.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nData types and missing values:\")\n",
    "display(df.info())\n",
    "\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "display(df.describe(include='all'))\n",
    "\n",
    "print(\"\\nUnique values in categorical columns:\")\n",
    "print(f\"Sex: {df['sex'].unique()}\")\n",
    "print(f\"Smoker: {df['smoker'].unique()}\")\n",
    "print(f\"Region: {df['region'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a015d0",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning & Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e26f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the original data\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "# Convert categorical columns to proper type\n",
    "cat_cols = ['sex', 'smoker', 'region']\n",
    "for col in cat_cols:\n",
    "    df_clean[col] = df_clean[col].astype('category')\n",
    "\n",
    "# Check for outliers in numerical columns\n",
    "num_cols = ['age', 'bmi', 'children', 'charges']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(num_cols, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.boxplot(y=df_clean[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Handle outliers in charges (target variable)\n",
    "Q1 = df_clean['charges'].quantile(0.25)\n",
    "Q3 = df_clean['charges'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Lower bound for charges: {lower_bound}\")\n",
    "print(f\"Upper bound for charges: {upper_bound}\")\n",
    "\n",
    "# We won't remove outliers in charges as they might represent genuine cases\n",
    "# But we'll apply log transformation to reduce skewness\n",
    "df_clean['log_charges'] = np.log1p(df_clean['charges'])\n",
    "\n",
    "# Create age groups for better analysis\n",
    "df_clean['age_group'] = pd.cut(df_clean['age'], \n",
    "                               bins=[0, 18, 30, 45, 60, 100],\n",
    "                               labels=['0-18', '19-30', '31-45', '46-60', '60+'])\n",
    "\n",
    "# Create BMI categories\n",
    "df_clean['bmi_category'] = pd.cut(df_clean['bmi'],\n",
    "                                 bins=[0, 18.5, 25, 30, 100],\n",
    "                                 labels=['Underweight', 'Normal', 'Overweight', 'Obese'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd89b9d",
   "metadata": {},
   "source": [
    "### 3. Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9cea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Charges (Original and Log Transformed)\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_clean['charges'], bins=50, kde=True)\n",
    "plt.title('Distribution of Insurance Charges')\n",
    "plt.xlabel('Charges ($)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_clean['log_charges'], bins=50, kde=True)\n",
    "plt.title('Distribution of Log-Transformed Charges')\n",
    "plt.xlabel('Log(Charges)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Charges by Categorical Features\n",
    "cat_features = ['sex', 'smoker', 'region', 'age_group', 'bmi_category']\n",
    "plt.figure(figsize=(18, 12))\n",
    "for i, feature in enumerate(cat_features, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.boxplot(x=feature, y='charges', data=df_clean)\n",
    "    plt.title(f'Charges by {feature}')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation Heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df_clean[num_cols].corr(), annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap of Numerical Features')\n",
    "plt.show()\n",
    "\n",
    "# Pairplot of Numerical Features\n",
    "sns.pairplot(df_clean[num_cols], diag_kind='kde')\n",
    "plt.suptitle('Pairplot of Numerical Features', y=1.02)\n",
    "plt.show()\n",
    " \n",
    "# Create the interactive 3D scatter plot using Plotly\n",
    "# 3D Scatter Plot of Age, BMI, and Charges\n",
    "fig = px.scatter_3d(df_clean, x='age', y='bmi', z='charges',\n",
    "                    color='smoker', size='children',\n",
    "                    hover_data=['sex', 'region'],\n",
    "                    title='3D Relationship: Age, BMI, and Charges')\n",
    "\n",
    "# Interactive 3D Scatter Plot (using plotly)\n",
    "fig = px.scatter_3d(df_clean, x='age', y='bmi', z='charges',\n",
    "                    color='smoker', size='children',\n",
    "                    hover_data=['sex', 'region'],\n",
    "                    title='3D Relationship: Age, BMI, and Charges')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743b91ff",
   "metadata": {},
   "source": [
    "### 4. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153df009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Testing: Do smokers have significantly higher charges?\n",
    "print(\"\\nT-test for Charges Difference Between Smokers and Non-Smokers:\")\n",
    "smoker_charges = df_clean[df_clean['smoker'] == 'yes']['charges']\n",
    "non_smoker_charges = df_clean[df_clean['smoker'] == 'no']['charges']\n",
    "t_stat, p_value = stats.ttest_ind(smoker_charges, non_smoker_charges, equal_var=False)\n",
    "print(f\"T-statistic: {t_stat:.2f}, p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    mean_diff = smoker_charges.mean() - non_smoker_charges.mean()\n",
    "    print(f\"Conclusion: Smokers have significantly {'higher' if mean_diff > 0 else 'lower'} charges (${mean_diff:,.2f} difference).\")\n",
    "else:\n",
    "    print(\"Conclusion: No significant difference in charges between smokers and non-smokers.\")\n",
    "\n",
    "# ANOVA Test for Charges Across BMI Categories\n",
    "print(\"\\nANOVA Test for Charges Across BMI Categories:\")\n",
    "bmi_groups = df_clean['bmi_category'].unique()\n",
    "charges_by_bmi = [df_clean[df_clean['bmi_category'] == bmi]['charges'] for bmi in bmi_groups]\n",
    "f_stat, p_value = stats.f_oneway(*charges_by_bmi)\n",
    "print(f\"F-statistic: {f_stat:.2f}, p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Conclusion: There are significant differences in charges across BMI categories.\")\n",
    "else:\n",
    "    print(\"Conclusion: No significant differences in charges across BMI categories.\")\n",
    "\n",
    "# Correlation between age and charges\n",
    "pearson_corr, pearson_p = stats.pearsonr(df_clean['age'], df_clean['charges'])\n",
    "print(f\"\\nPearson Correlation between Age and Charges: {pearson_corr:.2f} (p-value: {pearson_p:.4f})\")\n",
    "\n",
    "# Chi-square Test for Smoking and Sex\n",
    "contingency_table = pd.crosstab(df_clean['sex'], df_clean['smoker'])\n",
    "chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "print(f\"\\nChi-square Test for Smoking and Sex:\")\n",
    "print(f\"Chi2-statistic: {chi2:.2f}, p-value: {p:.4f}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    print(\"Conclusion: There is a significant association between sex and smoking status.\")\n",
    "else:\n",
    "    print(\"Conclusion: No significant association between sex and smoking status.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfdf5aa",
   "metadata": {},
   "source": [
    "### 5. Feature Engineering & Preprocessing for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d94bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "# Target variable: charges (we'll use both original and log-transformed for comparison)\n",
    "# Features: All other relevant columns\n",
    "\n",
    "# Select features and target\n",
    "features = ['age', 'sex', 'bmi', 'children', 'smoker', 'region']\n",
    "target = 'charges'\n",
    "log_target = 'log_charges'\n",
    "\n",
    "# Separate features and targets\n",
    "X = df_clean[features]\n",
    "y = df_clean[target]\n",
    "y_log = df_clean[log_target]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "_, _, y_train_log, y_test_log = train_test_split(X, y_log, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "numeric_features = ['age', 'bmi', 'children']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['sex', 'smoker', 'region']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Feature selection\n",
    "selector = SelectKBest(score_func=f_regression, k='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6325ea31",
   "metadata": {},
   "source": [
    "### 6. Machine Learning Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0711b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to evaluate\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(),\n",
    "    'Lasso Regression': Lasso(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'Support Vector Regression': SVR()\n",
    "}\n",
    "\n",
    "# Evaluate each model on both original and log-transformed targets\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Create pipeline for original target\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('selector', selector),\n",
    "        ('model', model)])\n",
    "    \n",
    "    # Create pipeline for log-transformed target\n",
    "    pipeline_log = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('selector', selector),\n",
    "        ('model', model)])\n",
    "    \n",
    "    # Fit and evaluate on original target\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Fit and evaluate on log-transformed target\n",
    "    pipeline_log.fit(X_train, y_train_log)\n",
    "    y_pred_log = pipeline_log.predict(X_test)\n",
    "    # Convert predictions back from log scale\n",
    "    y_pred_exp = np.expm1(y_pred_log)\n",
    "    mse_log = mean_squared_error(y_test, y_pred_exp)\n",
    "    rmse_log = np.sqrt(mse_log)\n",
    "    mae_log = mean_absolute_error(y_test, y_pred_exp)\n",
    "    r2_log = r2_score(y_test, y_pred_exp)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'Original Target': {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R2': r2\n",
    "        },\n",
    "        'Log Target': {\n",
    "            'RMSE': rmse_log,\n",
    "            'MAE': mae_log,\n",
    "            'R2': r2_log\n",
    "        },\n",
    "        'model': pipeline,\n",
    "        'model_log': pipeline_log\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Original - RMSE: {rmse:,.2f}, MAE: {mae:,.2f}, R2: {r2:.2f}\")\n",
    "    print(f\"  Log Trans - RMSE: {rmse_log:,.2f}, MAE: {mae_log:,.2f}, R2: {r2_log:.2f}\")\n",
    "    print()\n",
    "\n",
    "# Compare model performance\n",
    "results_df = pd.DataFrame.from_dict({(i,j): results[i][j] \n",
    "                                   for i in results.keys() \n",
    "                                   for j in results[i].keys() \n",
    "                                   if j in ['Original Target', 'Log Target']},\n",
    "                                  orient='index')\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "display(results_df)\n",
    "\n",
    "# Visualize model performance\n",
    "plt.figure(figsize=(14, 6))\n",
    "results_df.reset_index().pivot(index='level_0', columns='level_1', values='RMSE').plot(kind='bar')\n",
    "plt.title('Model Comparison by RMSE (Lower is Better)')\n",
    "plt.ylabel('Root Mean Squared Error (RMSE)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c26f2f",
   "metadata": {},
   "source": [
    "### 7. Hyperparameter Tuning for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1bce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the best performing model (Gradient Boosting with log transform in this case)\n",
    "best_model_name = 'Gradient Boosting'\n",
    "best_target_type = 'Log Target'\n",
    "print(f\"\\nPerforming hyperparameter tuning for {best_model_name} ({best_target_type})...\")\n",
    "\n",
    "# Define parameter grid for Gradient Boosting\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__learning_rate': [0.05, 0.1],\n",
    "    'model__max_depth': [3, 5],\n",
    "    'model__min_samples_split': [2, 5],\n",
    "    'model__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Create pipeline for GridSearchCV (using log-transformed target)\n",
    "pipeline_log = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', selector),\n",
    "    ('model', GradientBoostingRegressor(random_state=42))])\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline_log, param_grid, cv=5, \n",
    "                          scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train_log)\n",
    "\n",
    "# Get best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate best model\n",
    "y_pred_log = best_model.predict(X_test)\n",
    "y_pred_exp = np.expm1(y_pred_log)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_exp))\n",
    "r2 = r2_score(y_test, y_pred_exp)\n",
    "\n",
    "print(f\"\\nBest Parameters: {best_params}\")\n",
    "print(f\"Tuned Model RMSE: {rmse:,.2f}\")\n",
    "print(f\"Tuned Model R2 Score: {r2:.2f}\")\n",
    "\n",
    "# Feature Importance from the best model\n",
    "if hasattr(best_model.named_steps['model'], 'feature_importances_'):\n",
    "    # Get feature names after preprocessing\n",
    "    numeric_features_transformed = numeric_features\n",
    "    categorical_features_transformed = best_model.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "    feature_names = np.concatenate([numeric_features_transformed, categorical_features_transformed])\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = best_model.named_steps['model'].feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(\"Feature Importances\")\n",
    "    plt.bar(range(len(importances)), importances[indices], color=\"skyblue\", align=\"center\")\n",
    "    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=90)\n",
    "    plt.xlim([-1, len(importances)])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
